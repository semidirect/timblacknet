<!doctype html><html lang="en"><head><meta charset="utf-8"/><link rel="icon" href="/led3d/favicon.ico"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="theme-color" content="#000000"/><meta name="description" content="This Christmas tree is strung with 900 individually addressable LED lights. Using data collected from an iPad camera, I wrote a program to compute the coordinates of the LEDs in each photograph and stitch this data together to form a 3D model of the positions of all the LEDs. What excites me about the project is that it’s a pure representation of data: each data point holds information from three features — its three coordinates in space, and unlike the pixels in a computer monitor or TV, each LED is not pre-destined to be placed in a particular position or fit into a rigid grid. The LED can organically go where it will, its location not to be prescribed but rather to be discovered. Furthermore, no single picture can give you the full positional information about each LED. Only by collecting data from multiple perspectives can we reconstruct the full model.

      With the coordinates determined, the tree plays 3D animations ranging from simple shapes to science simulations to emojis. Keep an eye out for diffusion-limited aggregation, a candy cane, the perceptron learning algorithm, a rain stick, and Conway's Game of Life."/><link rel="apple-touch-icon" href="/led3d/logo192.png"/><link rel="manifest" href="/led3d/manifest.json"/><title>LED3D</title><script defer="defer" src="/led3d/static/js/main.6984412b.js"></script><link href="/led3d/static/css/main.a38908d8.css" rel="stylesheet"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div></body></html>